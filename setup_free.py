#!/usr/bin/env python3
"""Setup script for free AI RAG Agent configuration."""

import os
import sys
import subprocess
from pathlib import Path
from typing import Dict, List


class FreeSetupManager:
    """Manages setup for free AI RAG Agent alternatives."""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.env_file = self.project_root / ".env"
        
    def check_python_version(self) -> bool:
        """Check if Python version is compatible."""
        if sys.version_info < (3, 8):
            print("Error: Python 3.8+ is required")
            return False
        print(f"Python {sys.version_info.major}.{sys.version_info.minor} detected")
        return True
    
    def install_dependencies(self, provider: str) -> bool:
        """Install dependencies for the chosen provider."""
        print(f"\nInstalling dependencies for {provider}...")
        
        try:
            # Install base requirements
            subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"], 
                         check=True, capture_output=True)
            print("Base dependencies installed")
            
            # Install provider-specific dependencies
            if provider == "gemini":
                subprocess.run([sys.executable, "-m", "pip", "install", 
                              "google-generativeai", "llama-index-llms-gemini", 
                              "llama-index-embeddings-gemini"], 
                             check=True, capture_output=True)
                print("Gemini dependencies installed")
                
            elif provider == "ollama":
                subprocess.run([sys.executable, "-m", "pip", "install", 
                              "llama-index-llms-ollama", "llama-index-embeddings-ollama"], 
                             check=True, capture_output=True)
                print("Ollama dependencies installed")
                
            # Always install HuggingFace for embeddings
            subprocess.run([sys.executable, "-m", "pip", "install", 
                          "llama-index-embeddings-huggingface", "sentence-transformers"], 
                         check=True, capture_output=True)
            print("HuggingFace embeddings installed")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"Failed to install dependencies: {e}")
            return False
    
    def check_ollama_installation(self) -> bool:
        """Check if Ollama is installed and running."""
        try:
            result = subprocess.run(["ollama", "--version"], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("Ollama is installed")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        print("Ollama not found")
        print("Install Ollama from: https://ollama.ai/")
        return False
    
    def check_ollama_running(self) -> bool:
        """Check if Ollama server is running."""
        try:
            import requests
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            if response.status_code == 200:
                print("Ollama server is running")
                return True
        except Exception:
            pass
        
        print("Ollama server not running")
        print("Start with: ollama serve")
        return False
    
    def setup_ollama_models(self) -> bool:
        """Download required Ollama models."""
        models = ["llama3.1:8b", "mxbai-embed-large"]
        
        for model in models:
            print(f"Downloading {model}...")
            try:
                result = subprocess.run(["ollama", "pull", model], 
                                      capture_output=True, text=True, timeout=300)
                if result.returncode == 0:
                    print(f"{model} downloaded")
                else:
                    print(f"Failed to download {model}: {result.stderr}")
                    return False
            except subprocess.TimeoutExpired:
                print(f"Download timeout for {model}")
                return False
            except Exception as e:
                print(f"Error downloading {model}: {e}")
                return False
        
        return True
    
    def create_env_file(self, config: Dict[str, str]) -> bool:
        """Create .env file with the provided configuration."""
        try:
            with open(self.env_file, "w") as f:
                f.write("# AI RAG Agent Configuration\n")
                f.write("# Generated by setup_free.py\n\n")
                
                for key, value in config.items():
                    f.write(f"{key}={value}\n")
            
            print(f"Configuration saved to {self.env_file}")
            return True
            
        except Exception as e:
            print(f"Failed to create .env file: {e}")
            return False
    
    def get_gemini_api_key(self) -> str:
        """Get Gemini API key from user."""
        print("\nGemini API Key Setup")
        print("1. Go to https://ai.google.dev/")
        print("2. Click 'Get API key'")
        print("3. Create a new API key")
        print("4. Copy the key and paste it below")
        
        while True:
            api_key = input("\nEnter your Gemini API key (or 'skip' to configure later): ").strip()
            
            if api_key.lower() == 'skip':
                return ""
            
            if len(api_key) > 20:  # Basic validation
                return api_key
            
            print("Invalid API key format. Please try again.")
    
    def setup_gemini(self) -> Dict[str, str]:
        """Setup Gemini configuration."""
        print("\nSetting up Gemini (Free)")
        
        api_key = self.get_gemini_api_key()
        
        config = {
            "LLM_PROVIDER": "gemini",
            "EMBEDDING_PROVIDER": "huggingface",
            "GEMINI_API_KEY": api_key,
            "GEMINI_MODEL": "gemini-1.5-flash",
            "EMBEDDING_MODEL": "BAAI/bge-small-en-v1.5",
            "CHROMA_PERSIST_DIRECTORY": "./data/chroma_db",
            "API_HOST": "0.0.0.0",
            "API_PORT": "8000",
            "LOG_LEVEL": "INFO"
        }
        
        return config
    
    def setup_ollama(self) -> Dict[str, str]:
        """Setup Ollama configuration."""
        print("\nSetting up Ollama (Completely Free)")
        
        if not self.check_ollama_installation():
            return {}
        
        if not self.check_ollama_running():
            print("\nPlease start Ollama server first: ollama serve")
            input("Press Enter when Ollama is running...")
            
            if not self.check_ollama_running():
                print("Ollama server still not accessible")
                return {}
        
        if not self.setup_ollama_models():
            print("Failed to download required models")
            return {}
        
        config = {
            "LLM_PROVIDER": "ollama",
            "EMBEDDING_PROVIDER": "huggingface",
            "OLLAMA_BASE_URL": "http://localhost:11434",
            "OLLAMA_MODEL": "llama3.1:8b",
            "EMBEDDING_MODEL": "BAAI/bge-small-en-v1.5",
            "CHROMA_PERSIST_DIRECTORY": "./data/chroma_db",
            "API_HOST": "0.0.0.0",
            "API_PORT": "8000",
            "LOG_LEVEL": "INFO"
        }
        
        return config
    
    def run_setup(self):
        """Run the complete setup process."""
        print("AI RAG Agent - Free Setup")
        print("=" * 40)
        
        if not self.check_python_version():
            return False
        
        print("\nChoose your free LLM provider:")
        print("1. Gemini (Free API, requires internet)")
        print("2. Ollama (Completely free, runs locally)")
        
        while True:
            choice = input("\nEnter your choice (1 or 2): ").strip()
            
            if choice == "1":
                provider = "gemini"
                config = self.setup_gemini()
                break
            elif choice == "2":
                provider = "ollama"
                config = self.setup_ollama()
                break
            else:
                print("Invalid choice. Please enter 1 or 2.")
        
        if not config:
            print("Setup failed")
            return False
        
        if not self.install_dependencies(provider):
            return False
        
        if not self.create_env_file(config):
            return False
        
        print("\nSetup completed successfully!")
        print("\nNext steps:")
        print("1. Run: python -m src serve")
        print("2. Run: python -m src crawl <url>")
        print("3. Run: python -m src query <question>")
        
        if provider == "gemini" and not config.get("GEMINI_API_KEY"):
            print("\nDon't forget to add your Gemini API key to .env file")
        
        return True


if __name__ == "__main__":
    setup_manager = FreeSetupManager()
    success = setup_manager.run_setup()
    sys.exit(0 if success else 1)